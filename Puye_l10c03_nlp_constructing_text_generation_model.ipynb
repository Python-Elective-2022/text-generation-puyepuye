{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244f7b40-30e0-4360-9ad8-8adfa2141aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-17 06:12:37--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.130.100, 74.125.130.101, 74.125.130.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.130.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9cib087d5cena25gfgjlb43dv5f1epca/1679033550000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=4e1af1fe-3fdd-4002-969d-9e46c2c00bfb [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-17 06:12:39--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9cib087d5cena25gfgjlb43dv5f1epca/1679033550000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=4e1af1fe-3fdd-4002-969d-9e46c2c00bfb\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  49.9MB/s    in 1.4s    \n",
            "\n",
            "2023-03-17 06:12:41 (49.9 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f028f0-19a0-4d01-c1ae-f90538933364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14290ea8-8e72-402a-c6c9-af905c59ed6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def10c6e-5dd7-4bcd-f061-f6cfcdc25ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 20s 70ms/step - loss: 6.0228 - accuracy: 0.0227\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 5.4435 - accuracy: 0.0328\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 2s 26ms/step - loss: 5.3625 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.3076 - accuracy: 0.0394\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 5.2329 - accuracy: 0.0464\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1540 - accuracy: 0.0545\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.0759 - accuracy: 0.0631\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9947 - accuracy: 0.0631\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9087 - accuracy: 0.0757\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8071 - accuracy: 0.0888\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7023 - accuracy: 0.0908\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5946 - accuracy: 0.0989\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.4882 - accuracy: 0.1095\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3919 - accuracy: 0.1221\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2942 - accuracy: 0.1307\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1971 - accuracy: 0.1539\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1057 - accuracy: 0.1559\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0093 - accuracy: 0.1726\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9237 - accuracy: 0.1968\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8368 - accuracy: 0.2154\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7607 - accuracy: 0.2301\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6826 - accuracy: 0.2513\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6127 - accuracy: 0.2644\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.5273 - accuracy: 0.2699\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.4465 - accuracy: 0.2921\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.3708 - accuracy: 0.3138\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.3103 - accuracy: 0.3249\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.2228 - accuracy: 0.3527\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.1581 - accuracy: 0.3673\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.1016 - accuracy: 0.3749\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.0582 - accuracy: 0.3895\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.9861 - accuracy: 0.4087\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9246 - accuracy: 0.4122\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8625 - accuracy: 0.4188\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8056 - accuracy: 0.4279\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.7571 - accuracy: 0.4304\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7056 - accuracy: 0.4369\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6703 - accuracy: 0.4501\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6715 - accuracy: 0.4435\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5906 - accuracy: 0.4622\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.5379 - accuracy: 0.4728\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4766 - accuracy: 0.4939\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4261 - accuracy: 0.4985\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.4008 - accuracy: 0.4975\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3503 - accuracy: 0.5055\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3020 - accuracy: 0.5192\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2489 - accuracy: 0.5308\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2130 - accuracy: 0.5383\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.1742 - accuracy: 0.5419\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.1404 - accuracy: 0.5580\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.0979 - accuracy: 0.5626\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.0590 - accuracy: 0.5782\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0530 - accuracy: 0.5777\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0149 - accuracy: 0.5873\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9773 - accuracy: 0.5858\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9277 - accuracy: 0.6115\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8936 - accuracy: 0.6130\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8631 - accuracy: 0.6171\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8301 - accuracy: 0.6206\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7972 - accuracy: 0.6246\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.7741 - accuracy: 0.6261\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7607 - accuracy: 0.6342\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7269 - accuracy: 0.6438\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6895 - accuracy: 0.6504\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6567 - accuracy: 0.6599\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.6619 - accuracy: 0.6509\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6764 - accuracy: 0.6488\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6046 - accuracy: 0.6756\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5758 - accuracy: 0.6741\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.5539 - accuracy: 0.6756\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5098 - accuracy: 0.6862\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5154 - accuracy: 0.6842\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5447 - accuracy: 0.6710\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4620 - accuracy: 0.6983\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 1.4309 - accuracy: 0.6988\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3990 - accuracy: 0.7094\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3785 - accuracy: 0.7195\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3617 - accuracy: 0.7195\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3396 - accuracy: 0.7235\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3181 - accuracy: 0.7341\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2936 - accuracy: 0.7351\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2706 - accuracy: 0.7417\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2586 - accuracy: 0.7356\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2388 - accuracy: 0.7462\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2135 - accuracy: 0.7497\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1929 - accuracy: 0.7619\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1722 - accuracy: 0.7634\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1519 - accuracy: 0.7679\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1444 - accuracy: 0.7679\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1239 - accuracy: 0.7770\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1143 - accuracy: 0.7760\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1010 - accuracy: 0.7785\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0854 - accuracy: 0.7830\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0806 - accuracy: 0.7790\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0543 - accuracy: 0.7836\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0560 - accuracy: 0.7856\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0354 - accuracy: 0.7881\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0174 - accuracy: 0.7967\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0083 - accuracy: 0.7997\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9950 - accuracy: 0.7977\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.9899 - accuracy: 0.7957\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.9802 - accuracy: 0.8032\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.9807 - accuracy: 0.7982\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.9659 - accuracy: 0.7982\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9489 - accuracy: 0.8073\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9500 - accuracy: 0.8052\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9742 - accuracy: 0.7952\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9168 - accuracy: 0.8073\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8915 - accuracy: 0.8199\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8795 - accuracy: 0.8244\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8781 - accuracy: 0.8234\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8531 - accuracy: 0.8274\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8404 - accuracy: 0.8310\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8271 - accuracy: 0.8385\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8155 - accuracy: 0.8325\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8025 - accuracy: 0.8370\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7929 - accuracy: 0.8401\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7834 - accuracy: 0.8406\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7858 - accuracy: 0.8355\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7835 - accuracy: 0.8365\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7877 - accuracy: 0.8345\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7660 - accuracy: 0.8370\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7526 - accuracy: 0.8375\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7355 - accuracy: 0.8446\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7316 - accuracy: 0.8461\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7473 - accuracy: 0.8325\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7264 - accuracy: 0.8396\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7072 - accuracy: 0.8466\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.8502\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6904 - accuracy: 0.8476\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6866 - accuracy: 0.8496\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.8471\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6720 - accuracy: 0.8502\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6720 - accuracy: 0.8527\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6693 - accuracy: 0.8481\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6565 - accuracy: 0.8491\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6598 - accuracy: 0.8481\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6616 - accuracy: 0.8436\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6487 - accuracy: 0.8542\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6266 - accuracy: 0.8572\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6203 - accuracy: 0.8557\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6101 - accuracy: 0.8633\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5988 - accuracy: 0.8658\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5914 - accuracy: 0.8633\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5857 - accuracy: 0.8648\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5818 - accuracy: 0.8628\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5770 - accuracy: 0.8638\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5691 - accuracy: 0.8643\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5659 - accuracy: 0.8658\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5591 - accuracy: 0.8693\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5575 - accuracy: 0.8688\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 0.5627 - accuracy: 0.8602\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5504 - accuracy: 0.8678\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5487 - accuracy: 0.8683\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5483 - accuracy: 0.8643\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5467 - accuracy: 0.8663\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5332 - accuracy: 0.8673\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5201 - accuracy: 0.8729\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5222 - accuracy: 0.8744\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5142 - accuracy: 0.8718\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5059 - accuracy: 0.8759\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5150 - accuracy: 0.8708\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5203 - accuracy: 0.8703\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.8724\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.8708\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4930 - accuracy: 0.8784\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.8779\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.8708\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4898 - accuracy: 0.8769\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4821 - accuracy: 0.8794\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4716 - accuracy: 0.8814\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4621 - accuracy: 0.8850\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4544 - accuracy: 0.8850\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.4535 - accuracy: 0.8855\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.4657 - accuracy: 0.8819\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4590 - accuracy: 0.8804\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4507 - accuracy: 0.8860\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4449 - accuracy: 0.8875\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4359 - accuracy: 0.8875\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4371 - accuracy: 0.8875\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4322 - accuracy: 0.8890\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4270 - accuracy: 0.8845\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4241 - accuracy: 0.8915\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.4213 - accuracy: 0.8890\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4185 - accuracy: 0.8905\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4215 - accuracy: 0.8935\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4144 - accuracy: 0.8910\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4086 - accuracy: 0.8925\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4314 - accuracy: 0.8835\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4809 - accuracy: 0.8744\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4894 - accuracy: 0.8734\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4586 - accuracy: 0.8794\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4355 - accuracy: 0.8870\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4268 - accuracy: 0.8875\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4107 - accuracy: 0.8930\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3990 - accuracy: 0.8956\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 0.3969 - accuracy: 0.8890\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3921 - accuracy: 0.8991\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3934 - accuracy: 0.8915\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3925 - accuracy: 0.8900\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "33f4902a-3d72-4472-8eaa-de750d15bf01"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b298737-f9c4-4bb1-9d7d-bca5bdbd86e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 651ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "im feeling chills me to do me sunrise other bone ive neighbours evening ways scars evening evening evening break reason only life i believe love all in you would weave what to do god do what god do what to do theyre on on on sun hour hour hour hour i would break shouting shouting park hour am can feel am am can know must must must must must must must must must must must rely never came down world stood go hes bridges lousy bone bone bone hope am am can would hope must am must guys must guys hope beg life\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}